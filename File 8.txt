naresh1a
i-07b4e7c2553fe492b
18.141.172.249

naresh1b
i-0af69b3dbf8871eb8
13.213.30.118


/////////// Deployment ////////////////

kubectl delete rs vote
kubectl delete svc vote

Create the Deployment
$ cat > vote-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vote
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 0
  revisionHistoryLimit: 4
  paused: false
  replicas: 8
  minReadySeconds: 20
  selector:
    matchLabels:
      role: vote
    matchExpressions:
      - {key: version, operator: In, values: [v1, v2, v3]}
  template:
    metadata:
      name: vote
      labels:
        app: python
        role: vote
        version: v2
    spec:
      containers:
        - name: app
          image: schoolofdevops/vote:v2
          ports:
            - containerPort: 80
              protocol: TCP
press ctrl+d
Create the deployment using the following command
$ kubectl apply -f vote-deploy.yaml
Create the service for the depoyment
cat > vote-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: vote
  labels:
    role: vote
spec:
  selector:
    role: vote
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30000
  type: NodePort
presse ctrl+d
Create the service 
$ kubectl apply -f vote-svc.yaml
to list the deployment,rs and pod
$ kubectl get deployment,rs,pod
$ kubectl get deployment
$ kubectl get rs
$ kubectl get pod 
To list te service
$ kubectl get svc
verify the node port and access the application using the http://nodeip:nodeport in your browser
Scaling the deployment
Modify the vote-deploy.yaml file and change the replica count to 4 . you can als use command line to scale the replicas
apply the yaml file
$ vi vote-deploy.yaml 
change the replica count to 4
$ kubectl apply -f vote-deploy.yaml
verify using 
$ kubectl get deployment
$ kubectl get pod
Performing rolling update
modify the vote-depoy.yaml file and change the pod label to v3 and image to v4
labels:
        app: python
        role: vote
        version: v3
    spec:
      containers:
        - name: app
          image: schoolofdevops/vote:v4
          ports:
            - containerPort: 80
              protocol: TCP
Apply the yaml file
$ kubectl apply -f vote-deploy.yaml
To verify that the upgrade started 
$ kubectl rollout status deployment vote
browse the application in the browser and refresh the page 2 or 2 times you will see old verion and new version of the application
$ kubectl get pod 
$ kubectl describe pod podname
verify the image name in the describe output and pod label
To rollback to the previous version if not satisfied with the update
$ kubectl rollout history deployment vote
it will diplay different revision of the update history
To check the changes in each version please use the command
$ kubectl rollout history deployment vote --revision=1
To rollback to the previous version (version=1)
$ kubectl rollout undo deployment vote --to-revision=1
to verify the update
$ kubectl rollout status deployment vote
to check the rollout history
$ kubectl rollout history deployment vote
to delete the deployment 
$ kubectl delete deployment vote
or
$ kubectl delete -f vote-deploy.yaml


//////////////////////////////////////////////

Pod Scheduling

kubectl get node
kubectl get node --show-labels
cat >demopod.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: tomee
    env: dev
    version: v1
  name: demopod
spec:
  containers:
  - image: nginx
    name: demopod
  nodeName: k8master
kubectl apply -f demopod.yaml 
kubectl get pod -o wide
you will see that the pod is runing on master node because we have use nodeName and specify the name of the master node on which the pod will run. 
schduler is not coming in scope when you specify nodename. 

check the node taint of the master node
kubectl describe node k8master
remove the taint
kubectl taint node k8master node-role.kubernetes.io/master-
check the taint status
kubectl describe node k8master

$ kubectl delete -f demopod.yaml
kubectl get node
Define the label to both the node
kubectl label node k8master cpu=gpu
kubectl label node k8worker1 disk=ssd
kubectl get node --show-labels
cat > demopod.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: tomee
    env: dev
    version: v1
  name: demopod
spec:
  containers:
  - image: nginx
    name: demopod
  nodeSelector:
    cpu: gpu
kubectl apply -f demopod.yaml 
 kubectl get pod -o wide
now the pod will run on k8master as we are using the nodeSelctor and the label. th pod will run on the node which mathes the node label
 
 
Login to the worker node and stop the kubelet service
$ sudo systemctl stop kubelet
Login to the master node and exeecute you will see after some time the worker node is  NotReady
kubectl get node -o wide

Login to the worker node again and start the kubelet service
$ sudo systemctl start kubelet
login to the master and check the node status
kubectl get node

Node Maintenance
kubectl get node

the command will bring the node to maintainence and disable the scheduling
-During drain the worker node, if standealone podes are there in worker node then these will be deleted. 
-If the pods are part of replicaset / deployment those will move to another node automatically.
 
kubectl drain k8worker1 --force --ignore-daemonsets
kubectl get node -o wide
kubectl get pod -o wide
 kubectl get node
 kubectl create deployment  testsched --image=nginx --replicas=4
kubectl get pod -o wide
 kubectl get node 
Remove the node out of maintainence 
kubectl uncordon k8worker1
kubectl get node 
 kubectl create deployment  testsched2 --image=nginx --replicas=4
 kubectl get pod -o wide
kubectl delete deployment testsched testsched2


///////////////////////////////////////////////////

Secret and config map

git clone https://github.com/mrbobbytables/k8s-intro-tutorials.git

cd k8s-intro-tutorials/configuration


open the browser and use the link https://github.com/mrbobbytables/k8s-intro-tutorials/tree/master/configuration

    follow the lab

/////////////// Must do /// Covers the entair main lab //////////////

Login to hub.docker.com 
Create the private repo with name testdocker
click on Repository and then create repository and select private registry
Login to the master node
mkdir webapp
echo "<h2>Docker and Kubernetes Rocks</h2>" > webapp/index.html
cd webapp
cat >Dockerfile
FROM ubuntu:16.04
RUN apt-get update \
   && apt-get install -y apache2
COPY index.html /var/www/html/
WORKDIR /var/www/html
CMD ["apachectl", "-D", "FOREGROUND"]
EXPOSE 80
sudo docker image build -t webserver:apache2 .

sudo docker image ls
sudo docker container run -d -P webserver:apache2
sudo docker container ls|more
Check the port number and use the ip of the master node and the port number in the format http://ipaddr:port in your wweb browser
sudo docker stop container_id
sudo docker container rm 9ba850f7cc54

sudo docker image tag webserver:apache2 dockerid/testdocker:apache2
use your docker login id instead of hamid
sudo docker login
sudo docker image push dockerid/testdocker:apache2
delete the images
$ sudo docker image rm dockerid/testdocker:apache2
$ sudo docker image rm webserver:apache2
Create the deployment yaml file
kubectl create deployment web --image=dockerid/testdocker:apache2 --replicas=2 --dry-run=client -o yaml
kubectl create deployment web --image=dockerid/testdocker:apache2 --replicas=2 --dry-run=client -o yaml > webdeploy.yaml
cat webdeploy.yaml 
 
kubectl apply -f webdeploy.yaml 
C
 kubectl get pod
you will see the pod status is in errimagepull because the image is not pulled from the docker private registry. we need to store the docker credential as secrets to get it work
kubectl describe pod podname
$ kubectl delete -f webdeploy.yaml
Create the docker registry secret
kubectl create secret docker-registry regcred --docker-server=https://index.docker.io/v1/ --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>
where:
<your-registry-server> is your Private Docker Registry FQDN. (https://index.docker.io/v1/ for DockerHub)
<your-name> is your Docker username.
<your-pword> is your Docker password.
<your-email> is your Docker email.
kubectl get secret regcred --output=yaml
onvert the secret data to a readable format:
kubectl get secret regcred --output="jsonpath={.data.\.dockerconfigjson}" | base64 --decode
Modify the webdeploy.yaml file to add the imagepullsecrets
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: web
  name: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: web
    spec:
      containers:
      - image: hamidulrahman82/testdocker:apache2
        name: testdocker
      imagePullSecrets:
      - name: regcred 
Apply the yaml file
$ kubectl apply -f webdeploy.yaml
the pod will be running state 

Create service yaml file for deployment
kubectl expose deployment web --type=NodePort --port=80  --dry-run=client -o yaml
kubectl expose deployment web --type=NodePort --port=80  --dry-run=client -o yaml >websvc.yaml
cat websvc.yaml 
kubectl apply -f websvc.yaml 
kubectl get svc
kubectl describe svc web
use the ip of the worker node and the nodeport in your browser and verify the application
